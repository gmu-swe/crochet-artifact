\title{CROCHET Artifact Guide}
\author{Jonhattan Bell \and Lu\'{i}s Pina}
\date{\today}

\documentclass[12pt]{article}

\renewcommand\thesection{\Alph{section}}

\usepackage{url}
\usepackage{hyperref}
\usepackage{color}
\usepackage{soul}

% Monotype font with bold for listings
%\renewcommand{\ttdefault}{pcr}

\newcommand{\command}[1]{\colorbox{black}{\texttt{\color{white}#1}}}

\begin{document}
\maketitle

\begin{abstract}
This is the paper's abstract \ldots
\end{abstract}

\section{Introduction}

This is the guide for the artifact for the paper TITLE.  This artifact consists
of two VMs which are set-up and provisioned using
Vagrant.\footnote{\url{https://www.vagrantup.com/}}  You should install Vagrant
and Virtualbox in your system.  We tested this artifact with Vagrant version XXX
and Virtualbox version XXX on GNU/Linux and Vagrant version YYY and Virtualbox
version YYY on MacOS.

The sections on this paper are \emph{named} A through XXX to avoid any confusion with
the sections on the paper, which are \emph{numbered} 1 through XXX.

Section~\ref{sec:structure} describes the structure of the artifact,
Section~\ref{sec:provision} describes how to provision each VM,
Section~\ref{sec:run} describes how to run each experiment,
Section~\ref{sec:tables} describes how to generate the tables on the paper from
the results, Section~\ref{sec:algorithm} describes how to match the algorithm in
Listings TODO with Crochet's code, and Section~\ref{sec:extract} explains how to
extract the artifact from its VM to run the experiments directly.

To \emph{``kick-the-tires''}, we recommend following:
Section~\ref{sec:structure}, Section~\ref{sec:provision},
Section~\ref{sec:quickrun}, and Section~\ref{sec:tables}.

\section{Structure of this artifact}
\label{sec:structure}

\subsection{Main VM}
\label{sec:main}

The first VM is the Main VM that runs all the experiments except Section~XXX on
the paper.  It uses Ubuntu 16.04 LTS as the underlying operating system, and you
can find it on directory XXX.  It's structure is as follows:

\begin{description}

    \item[Vagrantfile] File that describes how Vagrant should provision the VM.
        Parameter \texttt{config.vm.provision}, at the end of the file,
        describes how to provision the artifact VM by running shell scripts
        listed below on this document.  Please customize the following
        parameters:

        \begin{description}

            \item[vm.memory] Memory that the VM will have.  We recommend at least 8G (8192MB).

            \item[vm.cpus] Number of CPUs available to the VM.  We recomment at least 2, more is better.

        \end{description}

    \item[downloads] The artifact requires some files to be downloaded from the
        internet and saved here.  We already populated this directory for your
        convenience.  File \texttt{scripts/downloads.sh} should be able to
        re-download them if needed.

        \begin{itemize}

            \item Oracle HotSpot JDK7

            \item Oracle HotSpot JDK8

            \item DaCapo 9.12 jar file

        \end{itemize}

    \item[scripts]  Several scripts and utilities used throughout the VM.  File
        \texttt{scripts/path.sh} is of particular interest as it sets the
        environment used throughout the artifact.  Ensure that variable
        \texttt{GLOBAL\_JVM\_PARAMS} configures JVMs with a maximum heap size
        that fits in the memory you configured the artifact VM with, in file
        \texttt{Vagrantfile} with parameter \texttt{vb.memory}.  We recommend at
        least 4GB with: \texttt{GLOBAL\_JVM\_PARAMS="-Xmx4G"}

    \item[experiments]  Each experiment reported in the paper has a
        subdirectory with the scripts to run it and generate a table with its
        results:

        \begin{description}

            \item[microbenchmarks] Section~5.1

            \item[dacapo] Section~5.2

            \item[stmbench7] Section~5.3, Table~3

            \item[ftp] Section~6.1

            \item[dacapo-h2] Section~6.2

        \end{description}

    \item[results]  The artifact saves the raw results of each experiment in
        this directory, under a subdirectory structure similar to the
        experiments structure.

    \item[tables]  Similarly to the results, the artifact saves in this
        directory the tables generated from the raw results.

\end{description}

\subsection{XJ VM}
\label{sec:xj}

The second VM only runs the experiments for Table~4 in Section~5.2 of the paper.

The structure of the VM is very similar to the Main VM, described in
Section~\ref{sec:main}.

\begin{description}

    \item[Vagrantfile] Similar file to the Main VM, please configure it as you
        did above.

    \item[downloads] The XJ VM requires the sources of the XJ JVM and the
        experiments it uses.  These files were kindly made available by the
        authors of XJ, and are saved here.

        \begin{itemize}

            \item openjdk.zip

            \item xj.zip

        \end{itemize}

        As for the Main VM, it also requires the JVMs used in testing:

        \begin{itemize}

            \item Oracle HotSpot JDK7

            \item Oracle HotSpot JDK8

        \end{itemize}

        Also, as for the Main VM, we already populated this folder with all the
        required files.

    \item[scripts]  Same as above, please configure it in a similar way.

    \item[patches]  Some patches required to build the required software on this VM.

    \item[experiments]  Only one experiment in this VM:

        \begin{description}

            \item[xj] Section~5.3, Table~4

        \end{description}

    \item[results]  Not used, moved to \texttt{\$HOME/results}.  See
        Section~\ref{sec:sync} for the reasons behind this decision.


    \item[tables]  Not used, moved to folder \texttt{\$HOME/tables}.  See
        Section~\ref{sec:sync} for the reasons behind this decision.

\end{description}

\subsection{Folder synchronization between host and VM}
\label{sec:sync}

Vagrant mounts the directory that contains file \texttt{Vagrantfile} under
\texttt{/vagrant} inside the VM.

On the Main VM, these two directories are synchronized in both directions:
Writes from the host are visible inside the VM, and vice-versa.  Saving the
results and tables in \texttt{/vagrant/results} and \texttt{/vagrant/tables}
ensures that you can access them from the host on folders \texttt{results} and
\texttt{tables} without needing to start the VM.

Because of this synchronization, running some experiments from inside
\texttt{/vagrant} will incur I/O overhead that skews results.  We recommend
running all experiments from \texttt{\$HOME}.

On the XJ VM, the two folders are synchronized only in one direction:  On VM
startup, Vagrant overwrites the contents of \texttt{/vagrant} with the contents
of the directory that contains file \texttt{Vagrantfile}.

For the results and tables not to be lost between VM cycling, they can be found
in folder \texttt{\$HOME/results} and \texttt{\$HOME/tables} inside the XJ VM.
Unfortunately, you have to copy them manually to the host machine (e.g., by
using \texttt{scp}).

\section{Provision the VM}
\label{sec:provision}

This section explains how to provision each VM.

Jon, create directory \texttt{repos} and run command
\command{../scripts/populateRepos.sh} from it to clone all the private repos
needed for the artifact. (delete this before sending the final version of this
guide)

\subsection{Main VM}

Once you have configure file \texttt{Vagrantfile} and have all required
downloaded files, as described in Section~\ref{sec:main}, simply run the command
\command{vagrant up} from the directory XXX.  Provisioning the Main VM took around 10
minutes on a machine equipped with XXX.

\subsection{XJ VM}

Similar to the Main VM, once you have configure file \texttt{Vagrantfile} and
have all required downloaded files, as described in Section~\ref{sec:xj}, simply
run the command \command{vagrant up} from the directory XXX.  Provisioning the
Main VM took around 30 minutes on a machine equipped with XXX.

\section{Run the experiments}
\label{sec:run}

To log-on a provisioned VM, use command \command{vagrant ssh}.  If a provisioned
machine is not running (either is stopped or suspended), you can start it with
command \command{vagrant up}.  To stop a running VM, log-on and type \command{sudo
poweroff}.  To suspend a running vm, use command \command{vagrant suspend}.

To run any experiment, you should log-on to the VM and type command
\texttt{source /vagrant/scripts/paths.sh} to set up the environment correctly.

Please note, for the Main VM, that folder \command{/vagrant} is the same folder
where you typed \command{vagrant up} on the host machine, and any changes between
these folders are synchronized.  As a result, results and tables generated
inside the VM are automatically available to the host machine.

Some experiments (e.g., DaCapo) write scratch results on the directory in which
they are launched from.  Therefore, to avoid directory synchronization overhead,
we recommend running the experiments from folder \texttt{\$HOME} on the
Main VM.

For the XJ VM, these two folders are only synchronized from the host to the VM
by copying the contents of the folder on the host at VM startup time.
Extracting the results requires other means (e.g., \command{scp}).

To stop a running experiment, interrupting the experiment script with
\texttt{CTRL+C} is not enough, as the current iteration will continue to execute
in the background.  You can stop it with command \command{killall -KILL java}.

All experiments run on the Main VM, except for experiments reported in Table~4.

\subsection{Quick run}
\label{sec:quickrun}

Quick runs are smaller versions of the regular experiment, ideal for the
\emph{kick-the-tires} phase of testing.  If these experiments complete
successfully, then the regular experiments should also do so.  In the following,
we describe how to run each quick experiment.

Note that quick experiments write their results in the same directory of the
regular experiments.  This is so that you can also test the scripts that
generate the tables from the data.  However, running a quick experiment will
overwrite any data left from the regular experiment.

\subsubsection{Main VM}

\begin{description}

    \item[microbenchmarks] Run command
        \command{/vagrant/experiments/micro/run-quick.py}.  This runs an
        experiment for each cell of Table 1 twice.  Running this set of
        experiments took around 30 minutes on a machine XXX.

        Once the experiment finishes, you can check that there are no errors
        ensuring that command \command{grep -Ri exception /vagrant/results/microbenchmark}
        does not print anything.

        Note that this quick run does not generate the full results, so the
        table generating scripts described in Section~\ref{sec:tables} will not
        work.  If you want to try those, set variable \texttt{GLOBAL\_TIMES} to
        2 in file \command{/vagrant/scripts/paths.sh}, reset the environment with
        command \command{source /vagrant/scripts/paths.sh} and do a paper run,
        described in Section~\ref{sec:paper-run}.

    \item[dacapo] Run command \command{/vagrant/experiments/dacapo/run-quick.py}.
        This runs an experiment for each cell of Table 2 twice.  Running this
        set of experiments took around 5 minutes on a machine XXX.

        Once the experiment finishes, you can check that there are no errors
        ensuring that command \command{grep -R FAIL /vagrant/results/dacapo} does not
        print anything.  Note that benchmark \texttt{tradesoap} may fail
        non-deterministically even for the native case, and is not included on
        the main results on Table XXX; please ignore errors from this benchmark.

    \item[stmbench7] Run command
        \command{/vagrant/experiments/stmbench7/run-quick.py}.  This runs an
        experiment for each cell of Table 3 twice.  Running this set of
        experiments took around 2 minutes on a machine XXX.

        Once the experiment finishes, you can check that there are no errors
        ensuring that command \command{grep -Ri exception /vagrant/results/stmbench7}
        does not print anything.

    \item[ftp] Run command
        \command{/vagrant/experiments/ftp/run-quick.py}.  This runs an
        experiment for each cell of Table 5 twice.  Running this set of
        experiments took around 2 minutes on a machine XXX.

        Once the experiment finishes, you can check that there are no errors
        ensuring that command \command{grep -Ri exception /vagrant/results/ftp}
        does not print anything.

    \item[dacapo-h2] Run command
        \command{/vagrant/experiments/dacapo-h2/run-quick.py}.  This runs the
        experiment describe in Section~6.2 twice.  Running this set of
        experiments took around 3 minutes on a machine XXX.

        Once the experiment finishes, you can check that there are no errors
        ensuring that command \command{grep -R FAIL /vagrant/results/dacapo-h2}
        does not print anything.

\end{description}

\subsubsection{XJ VM}

\begin{description}

    \item[xj] Run command \command{/vagrant/experiments/xj/run-quick.py}.  This
        runs an experiment for each cell of Table 4 twice.  Running this set of
        experiments took around 3 minutes on a machine XXX.

        Once the experiment finishes, you can check that there are no errors
        ensuring that commands \command{grep -Ri exception \$HOME/results/xj} and
        \command{grep -Ri differ \$HOME/results/xj} do not print anything.

\end{description}

\subsection{Paper run}
\label{sec:paper-run}

Running the full set of results used for the paper is similar to running the
quick results, described above.  The difference is that you will use scripts
\texttt{run.py} instead of \texttt{run-quick.py} for each experiment.

In the following, we provide the estimated time to run each experiment on a
machine YYY with the Main VM extracted, as described in
Section~\ref{sec:extract}.  We ran the \emph{xj} experiment inside the XJ VM.

\begin{description}

    \item[microbenchmark] Around 25h

    \item[dacapo] Around 8h.  Depending on your setup, benchmark \texttt{h2} may
        fail to converge.  When this happens, instead of it reporting a result,
        it prints the following error:~\emph{Benchmark failed to converge}.
        Extracting the artifact from the VM and pinning the benchmark to
        specific cores, as described in Section~\ref{sec:extract}, should fix
        this problem.

    \item[stmbench7] TODO

    \item[ftp] TODO

    \item[xj] TODO

\end{description}

\section{Generate result tables}
\label{sec:tables}

The directory containing the scripts to run each experiment --- \texttt{run.py}
and \texttt{run-quick.py} --- also contain scripts to generate the results
tables found in Section 5 and 6.  The name of these scripts start with
\texttt{table}.  For instance, to generate the tables for experiment
\texttt{dacapo} run command\linebreak
\command{/vagrant/experiments/dacapo/table-dacapo.py}.

Each table script reads the results from the environment variable set up by
\texttt{scripts/paths.sh}, processes them, and generates tables in CSV format in
folder \texttt{tables}.

We generated the tables on the paper manually from the CSV files, importing them
first to spreadsheet software to help us in this task.  We recommend you do the
same to visualize the results more easily.

TODO map CSV cols to paper table cols

\section{Algorithm location}
\label{sec:algorithm}

The code that implements the pseudo-code in Section~TODO can be found in folder
\texttt{\$HOME/software/crochet/TODO}.  As explained in the paper, each class
has two states:  \texttt{SLOW} or \texttt{FAST}.  The code for checkpoint,
rollback, and read/write for each state can be found in methods TODO,
respectively.

\section{Run the artifact outside of the VM}
\label{sec:extract}

The XJ VM is not possible to run outside of its VM because the version of
OpenJDK does not build successfully on modern systems.  We were able to build a
version of XJ by working around the compiler errors, only to have it allocate
all the memory available when code JIT compiling is on.

We followed the following approach to execute the experiments provided by the
Main VM directly on our test machine:

\begin{enumerate}

    \item In file \texttt{scripts/paths.sh}, configure all the build and install
        directories to reflect where you are building the artifact on your
        machine.

    \item Source file \texttt{scripts/paths.sh} so set up the environment.  Note
        that you will need to source this file after each time you modify it.

    \item The provisioning entry-point can be found at the end of file
        \texttt{Vagrantfile}.  Follow those steps until you reach the invocation
        of script \texttt{build-all.sh}. The provisioning uses \texttt{apt-get}
        as the package manager, you need to change this to the package manager
        your distribution uses to install the same packages.

    \item Follow each build script in folder \texttt{scripts/build} by the same
        order in which script \texttt{scripts/build-all.sh} calls them.  Modify
        the package manager commands and ensure all other commands work.  We
        advice going one file at the time, instead of blindly executing
        \texttt{scripts/build-all.sh} to build everything in one go.

    \item Adapt and execute all the commands in file
        \texttt{scripts/pre-run.sh}.

\end{enumerate}

By now, you should have the artifact deployed directly on your machine.  In each
experiment script \texttt{run.py}, there is a variable \texttt{wrap} that
defines a command to wrap the execution of each experiment.  This is a good
place to place commands to pin process to particular CPUs (e.g.,
\texttt{taskset} or \texttt{numactl}).

Pinning the experiments on cores on the same CPU ensures good quality results,
with low noise.  We strongly recommend doing so, especially for NUMA machines.

\end{document}

